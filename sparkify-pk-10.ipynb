{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']='AKIAUFL7YDLGEO7S57FW'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='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'\n",
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6 (default, Feb 26 2020, 20:54:15) \n",
      "[GCC 7.3.1 20180712 (Red Hat 7.3.1-6)] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "20/06/10 17:26:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/06/10 17:26:36 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.hadoop.security.AccessControlException: Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2447)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1159)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1156)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1173)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1148)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1961)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:620)\n",
      "\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:427)\n",
      "\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:864)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:178)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy13.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy14.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)\n",
      "\t... 26 more\n",
      "20/06/10 17:26:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "20/06/10 17:26:36 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n",
      "20/06/10 17:26:36 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "20/06/10 17:26:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/06/10 17:26:36 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.hadoop.security.AccessControlException: Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2447)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1159)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1156)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1173)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1148)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1961)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:620)\n",
      "\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:427)\n",
      "\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:864)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:178)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy13.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy14.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)\n",
      "\t... 26 more\n",
      "20/06/10 17:26:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "20/06/10 17:26:36 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n",
      "/usr/lib/spark/python/pyspark/shell.py:45: UserWarning: Failed to initialize Spark session.\n",
      "  warnings.warn(\"Failed to initialize Spark session.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/shell.py\", line 41, in <module>\n",
      "    spark = SparkSession._create_shell_session()\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 583, in _create_shell_session\n",
      "    return SparkSession.builder.getOrCreate()\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/lib/spark/python/pyspark/context.py\", line 375, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/lib/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/lib/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/lib/spark/python/pyspark/context.py\", line 314, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": org.apache.hadoop.security.AccessControlException: Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2474)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2447)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1159)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1156)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1173)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1148)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1961)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:620)\n",
      "\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:427)\n",
      "\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:864)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:178)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=emr-notebook, access=WRITE, inode=\"/user\":hdfs:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3049)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy13.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy14.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)\n",
      "\t... 26 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./bin/pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Return a spark session by getting an existing or creating a new one\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://sparkify-pk-10/\"\n",
    "\n",
    "\n",
    "song_data = os.path.join(input_data, \"song_data/*/*/*/*.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008502006530761719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session_create = time.time()\n",
    "spark = create_spark_session()\n",
    "print(time.time()-session_create)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "song_schema = StructType([StructField('num_songs', IntegerType()),\n",
    "                              StructField('artist_id', StringType()),\n",
    "                              StructField('artist_latitude', DoubleType()),\n",
    "                              StructField('artist_longitude', DoubleType()),\n",
    "                              StructField('artist_location', StringType()),\n",
    "                              StructField('artist_name', StringType()),\n",
    "                              StructField('song_id', StringType()),\n",
    "                              StructField('title', StringType()),\n",
    "                              StructField('duration', DoubleType()),\n",
    "                              StructField('year', IntegerType())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.7598741054535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_create = time.time()\n",
    "df = spark.read.json(song_data, schema=song_schema)\n",
    "print(time.time()-df_create)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "\n",
    "songs_output_file = output_data + \"songs.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_write = time.time()\n",
    "songs_table.write.partitionBy('year', 'artist_id').parquet(songs_output_file, mode='overwrite')\n",
    "print(time.time()-df_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').\\\n",
    "        withColumnRenamed('artist_name', 'name').\\\n",
    "        withColumnRenamed('artist_location', 'location').\\\n",
    "        withColumnRenamed('artist_latitude', 'latitude').\\\n",
    "        withColumnRenamed('artist_longitude', 'longitude').distinct()\n",
    "\n",
    "artists_output_file = output_data + \"artists.parquet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_write = time.time()\n",
    "artists_table.write.parquet(artists_output_file, mode='overwrite')\n",
    "print(time.time()-df_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data =os.path.join(input_data,\"log_data/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7647550106048584\n"
     ]
    }
   ],
   "source": [
    "df_read = time.time()\n",
    "df = spark.read.json(log_data)\n",
    "print(time.time()-df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(df.page == 'NextSong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').\\\n",
    "        withColumnRenamed('userId', 'user_id').\\\n",
    "        withColumnRenamed('firstName', 'first_name').\\\n",
    "        withColumnRenamed('lastName', 'last_name').\\\n",
    "        withColumnRenamed('gender', 'gender').\\\n",
    "        withColumnRenamed('level', 'level').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_output_file = output_data + \"users.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_write = time.time()\n",
    "users_table.write.parquet(user_output_file)\n",
    "print(time.time()-df_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda ts: datetime.utcfromtimestamp(ts/1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('timestamp', get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda ts: datetime.utcfromtimestamp(ts/1000.0), TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('datetime', get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26', timestamp='2018-11-15 00:30:26', datetime='2018-11-15 00:30:26')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_week_day = udf(lambda ts: datetime.utcfromtimestamp(ts/1000.0).weekday(), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = df.withColumn(\"hour\", hour(col(\"timestamp\"))).\\\n",
    "        withColumn(\"day\", dayofmonth(col(\"timestamp\"))).\\\n",
    "        withColumn(\"week\", weekofyear(col(\"timestamp\"))).\\\n",
    "        withColumn(\"month\", month(col(\"timestamp\"))).\\\n",
    "        withColumn(\"year\", year(col(\"timestamp\"))).\\\n",
    "        withColumn(\"weekday\", get_week_day(col(\"ts\"))).\\\n",
    "        select(col(\"timestamp\").alias(\"start_time\"),\n",
    "               col(\"hour\"),\n",
    "               col(\"day\"),\n",
    "               col(\"week\"),\n",
    "               col(\"month\"),\n",
    "               col(\"year\"),\n",
    "               col(\"weekday\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(start_time='2018-11-15 00:30:26', hour=0, day=15, week=46, month=11, year=2018, weekday=3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_output_file = output_data + \"time.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_output_file = output_data + \"time.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.304982423782349\n"
     ]
    }
   ],
   "source": [
    "df_write = time.time()\n",
    "time_table.write.partitionBy('year', 'month').parquet(time_output_file, mode='overwrite')\n",
    "print(time.time()-df_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_parquet_file = output_data + \"songs.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_parquet_file = output_data + \"songs.parquet\"\n",
    "song_df = spark.read.parquet(songs_parquet_file)\n",
    "artists_parquet_file = output_data + \"artists.parquet\"\n",
    "artists_df = spark.read.parquet(artists_parquet_file).drop('location')\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "df_songs_join_df = df.join(song_df, (song_df.title == df.song)).drop('artist_id')\n",
    "df_artists_songs_join_df = df_songs_join_df.join(artists_df, (df_songs_join_df.artist == artists_df.name))\n",
    "\n",
    "songplays_df = df_artists_songs_join_df.join(time_table,\n",
    "                                             df_artists_songs_join_df.ts == time_table.start_time,\n",
    "                                             'left').drop(df_artists_songs_join_df.year)\n",
    "songplays_table = songplays_df.select(\n",
    "    col('start_time'),\n",
    "    col('userId').alias('user_id'),\n",
    "    col('level'),\n",
    "    col('song_id'),\n",
    "    col('artist_id'),\n",
    "    col('sessionId').alias('session_id'),\n",
    "    col('location'),\n",
    "    col('userAgent').alias('user_agent'),\n",
    "    col('year'),\n",
    "    col('month')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "|start_time|user_id|level|song_id|artist_id|session_id|location|user_agent|year|month|\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_output_file = output_data + \"songplays.parquet\"\n",
    "songplays_table.write.partitionBy('year', 'month').parquet(songplays_output_file, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays_table.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays_table.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
